# ğŸ¤ AI Voice Assistant with PDF Intelligence

**ğŸš€ Talk to your documents! Upload PDFs and have natural voice conversations powered by AI.**

> A sophisticated voice-enabled conversational AI assistant that processes PDF documents and provides intelligent responses through speech interaction. Built with React, FastAPI, Groq Whisper, and Google Gemini.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![React](https://img.shields.io/badge/React-18+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green.svg)
![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)

## ğŸ“‹ Table of Contents

- [ğŸŒŸ Features](#-features)
- [ğŸ“Š System Flow](#-system-flow)
- [ğŸ› ï¸ Technology Stack](#ï¸-technology-stack)
- [âš¡ Quick Start](#-quick-start)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ¯ API Endpoints](#-api-endpoints)
- [ğŸ¤– AI Capabilities](#-ai-capabilities)
- [ğŸ¨ UI Components](#-ui-components)
- [ğŸ”„ State Management](#-state-management)
- [ğŸ“± Screenshots](#-screenshots)
- [ğŸš€ Deployment](#-deployment)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

## ğŸŒŸ Features

### ğŸ¯ Core Capabilities
- **ğŸ“„ PDF Processing**: Upload and parse PDF documents with intelligent text extraction
- **ğŸ¤ Voice Input**: Real-time speech-to-text using Groq Whisper API
- **ğŸ”Š Voice Output**: Natural text-to-speech using Google TTS (gTTS)
- **ğŸ§  AI Conversations**: Intelligent responses powered by Google Gemini and Groq LLMs
- **ğŸ“š RAG Pipeline**: Retrieval-Augmented Generation with ChromaDB vector storage
- **ğŸ“ Interactive Interviews**: AI-powered interview sessions based on document content
- **ğŸ’¬ Conversational Flow**: Contextual multi-turn conversations

### ğŸ¨ User Experience
- **âœ¨ Modern UI**: Beautiful, responsive interface with glassmorphism design
- **ğŸ­ Visual Feedback**: Dynamic animations and state indicators
- **ğŸŒˆ Color-coded States**: Blue (idle), Red (recording), Purple (thinking)
- **ğŸ“± Mobile-first**: Optimized for touch devices and desktop
- **ğŸŒ™ Dark Theme**: Elegant dark mode interface

## ğŸ“Š System Flow

### ğŸ”„ Complete Interaction Flow

```mermaid
sequenceDiagram
    participant User
    participant Frontend
    participant API
    participant STT as Groq Whisper
    participant LLM as Gemini/Groq
    participant TTS as Google TTS
    participant DB as ChromaDB
    
    Note over User,DB: PDF Upload Flow
    User->>Frontend: Upload PDF
    Frontend->>API: POST /upload/upload_pdf/
    API->>DB: Store embeddings
    API->>Frontend: Return file ID
    
    Note over User,DB: Voice Conversation Flow
    User->>Frontend: Press mic button
    Frontend->>Frontend: Start recording
    User->>Frontend: Speak question
    Frontend->>Frontend: Stop recording
    
    Frontend->>STT: Audio file
    STT->>Frontend: Transcript
    
    Frontend->>API: POST /flow/ask
    API->>DB: Retrieve context
    DB->>API: Relevant chunks
    API->>LLM: Generate response
    LLM->>API: AI response
    
    API->>TTS: Text to speech
    TTS->>API: Audio blob
    API->>Frontend: Response + Audio
    Frontend->>User: Play audio response
```

### ğŸ¯ State Management Flow

```mermaid
stateDiagram-v2
    [*] --> Idle: App Start
    
    Idle --> UploadingPDF: Click Upload
    UploadingPDF --> PDFReady: Upload Success
    UploadingPDF --> Idle: Upload Error
    
    PDFReady --> Listening: Click Mic
    Listening --> Processing: Stop Recording
    Processing --> Thinking: STT Complete
    Thinking --> Speaking: LLM Response
    Speaking --> PDFReady: Audio Complete
    
    PDFReady --> InterviewMode: Start Interview
    InterviewMode --> InterviewActive: Begin Questions
    InterviewActive --> InterviewActive: Q&A Loop
    InterviewActive --> PDFReady: End Interview
    
    state Processing {
        [*] --> AudioCapture
        AudioCapture --> SpeechToText
        SpeechToText --> [*]
    }
    
    state Thinking {
        [*] --> RetrieveContext
        RetrieveContext --> GenerateResponse
        GenerateResponse --> [*]
    }
    
    state Speaking {
        [*] --> TextToSpeech
        TextToSpeech --> PlayAudio
        PlayAudio --> [*]
    }
```

## ğŸ› ï¸ Technology Stack

### ğŸ¨ Frontend
- **React 18+** - Modern UI framework with hooks
- **Vite** - Fast build tool and dev server
- **Tailwind CSS** - Utility-first CSS framework
- **Web Audio API** - Real-time audio recording
- **MediaRecorder API** - Audio capture and processing

### âš¡ Backend
- **FastAPI** - High-performance Python web framework
- **Python 3.10+** - Modern Python with type hints
- **SQLAlchemy** - SQL ORM for database operations
- **MySQL** - Relational database for metadata
- **ChromaDB** - Vector database for embeddings
- **Cloudinary** - Cloud storage for PDF files

### ğŸ¤– AI & ML
- **Groq Whisper** - Ultra-fast speech-to-text
- **Google Gemini** - Advanced language model
- **Groq LLaMA** - Alternative LLM for diversity
- **Google TTS (gTTS)** - Natural speech synthesis
- **Sentence Transformers** - Text embeddings
- **LangChain** - LLM application framework

### ğŸ”§ DevOps & Tools
- **Docker** - Containerization (optional)
- **Git** - Version control
- **ESLint** - JavaScript linting
- **Prettier** - Code formatting

## âš¡ Quick Start

### ğŸ“‹ Prerequisites

```bash
# Required software
- Node.js 18+ and npm
- Python 3.10+
- MySQL 8.0+
- Git
```

### ğŸš€ Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ai-voice-assistant.git
cd ai-voice-assistant
```

2. **Setup Backend**
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment variables
cp .env.example .env
# Edit .env with your API keys and database config
```

3. **Setup Frontend**
```bash
cd ../frontend

# Install dependencies
npm install

# Setup environment (optional)
cp .env.example .env.local
```

4. **Configure Environment Variables**

Create `backend/.env`:
```env
# API Keys
GROQ_API_KEY=your_groq_api_key_here
GOOGLE_API_KEY=your_google_api_key_here

# Database
DATABASE_URL=mysql+mysqlconnector://user:password@localhost:3306/ai_assistant

# Cloudinary (for file storage)
CLOUDINARY_CLOUD_NAME=your_cloud_name
CLOUDINARY_API_KEY=your_api_key
CLOUDINARY_API_SECRET=your_api_secret
```

5. **Start the Application**

```bash
# Terminal 1: Start Backend
cd backend
source venv/bin/activate
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Terminal 2: Start Frontend
cd frontend
npm run dev
```

6. **Access the Application**
- Frontend: http://localhost:5173
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs

## ğŸ“ Project Structure

```
ğŸ“¦ ai-voice-assistant/
â”œâ”€â”€ ğŸ“ backend/
â”‚   â”œâ”€â”€ ğŸ“ db/                    # Database configuration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # Base model
â”‚   â”‚   â””â”€â”€ session.py           # Database session
â”‚   â”œâ”€â”€ ğŸ“ models/               # Data models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ pdf.py              # PDF model
â”‚   â”œâ”€â”€ ğŸ“ routers/             # API routes
â”‚   â”‚   â”œâ”€â”€ conversation.py     # Conversation endpoints
â”‚   â”‚   â”œâ”€â”€ flow.py            # Main flow controller
â”‚   â”‚   â”œâ”€â”€ interview.py       # Interview system
â”‚   â”‚   â”œâ”€â”€ qa.py             # Q&A endpoints
â”‚   â”‚   â””â”€â”€ uploads.py        # File upload
â”‚   â”œâ”€â”€ ğŸ“ services/           # Business logic
â”‚   â”‚   â”œâ”€â”€ conversational_interview.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py     # Text embeddings
â”‚   â”‚   â”œâ”€â”€ interview.py      # Interview logic
â”‚   â”‚   â”œâ”€â”€ llm.py           # Language model integration
â”‚   â”‚   â”œâ”€â”€ orchestrator.py  # Request orchestration
â”‚   â”‚   â”œâ”€â”€ pdf_reader.py    # PDF processing
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py  # RAG implementation
â”‚   â”‚   â””â”€â”€ vectorstore.py   # Vector database
â”‚   â”œâ”€â”€ ğŸ“ stt_services/      # Speech-to-Text
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ whisper_model.py
â”‚   â”œâ”€â”€ ğŸ“ tts_service/       # Text-to-Speech
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ tts_model.py
â”‚   â”œâ”€â”€ ğŸ“ utils/             # Utilities
â”‚   â”‚   â””â”€â”€ cloudinary.py    # File storage
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ ğŸ“ frontend/
â”‚   â”œâ”€â”€ ğŸ“ public/           # Static assets
â”‚   â”œâ”€â”€ ğŸ“ src/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ components/   # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ GalaxyDots.jsx    # Orb visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ MicButton.jsx     # Microphone control
â”‚   â”‚   â”‚   â”œâ”€â”€ Orb.jsx          # Main orb component
â”‚   â”‚   â”‚   â”œâ”€â”€ Subtitle.jsx     # Text display
â”‚   â”‚   â”‚   â””â”€â”€ UploadButton.jsx # File upload
â”‚   â”‚   â”œâ”€â”€ ğŸ“ hooks/        # React hooks
â”‚   â”‚   â”‚   â””â”€â”€ useVoiceAssistant.js # Main logic
â”‚   â”‚   â”œâ”€â”€ ğŸ“ utils/        # Utilities
â”‚   â”‚   â”‚   â””â”€â”€ api.js       # API client
â”‚   â”‚   â”œâ”€â”€ App.jsx          # Main app component
â”‚   â”‚   â”œâ”€â”€ App.css          # Styles and animations
â”‚   â”‚   â””â”€â”€ main.jsx         # Entry point
â”‚   â”œâ”€â”€ package.json         # Node dependencies
â”‚   â””â”€â”€ vite.config.js       # Build configuration
â””â”€â”€ README.md               # This file
```

## ğŸ”§ Configuration

### ğŸ”‘ API Keys Setup

1. **Groq API Key** (for Whisper STT)
   - Visit: https://console.groq.com/
   - Create account and generate API key
   - Add to `.env`: `GROQ_API_KEY=your_key_here`

2. **Google API Key** (for Gemini LLM)
   - Visit: https://aistudio.google.com/
   - Create project and enable Gemini API
   - Add to `.env`: `GOOGLE_API_KEY=your_key_here`

3. **Cloudinary** (for file storage)
   - Visit: https://cloudinary.com/
   - Create account and get credentials
   - Add to `.env`: Cloud name, API key, and secret

4. **MySQL Database**
   - Install MySQL 8.0+
   - Create database: `ai_assistant`
   - Update connection string in `.env`

### ğŸ—„ï¸ Database Setup

```sql
-- Create database
CREATE DATABASE ai_assistant CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- Create user (optional)
CREATE USER 'ai_user'@'localhost' IDENTIFIED BY 'your_password';
GRANT ALL PRIVILEGES ON ai_assistant.* TO 'ai_user'@'localhost';
FLUSH PRIVILEGES;
```

## ğŸ¯ API Endpoints

### ğŸ“¤ Upload Endpoints
- `POST /upload/upload_pdf/` - Upload PDF document
- `GET /upload/config_status` - Check Cloudinary configuration

### ğŸ’¬ Conversation Endpoints
- `POST /flow/ask` - Main conversation endpoint
- `POST /qa/ask` - Simple Q&A
- `GET /qa/status/{file_id}` - Check processing status

### ğŸ“ Interview Endpoints
- `POST /interview/start` - Start interview session
- `POST /interview/answer` - Submit interview answer
- `GET /interview/status/{session_id}` - Interview status

### ğŸ¤ Audio Endpoints
- `POST /api/v1/stt` - Speech-to-text conversion
- `POST /api/v1/tts` - Text-to-speech synthesis

### ğŸ¥ Health Endpoints
- `GET /` - API health check
- `GET /db/health` - Database health check

## ğŸ¤– AI Capabilities

### ğŸ§  Intelligent Features

1. **ğŸ“š Document Understanding**
   - PDF text extraction and chunking
   - Semantic embeddings with Sentence Transformers
   - Context-aware retrieval

2. **ğŸ’­ Conversational AI**
   - Multi-turn conversations with memory
   - Intent recognition and routing
   - Contextual response generation

3. **ğŸ“ Interview System**
   - Dynamic question generation
   - Difficulty level adaptation
   - Topic focus customization

4. **ğŸ” RAG Pipeline**
   - Efficient similarity search
   - Relevant context retrieval
   - Source attribution

### ğŸ›ï¸ Model Configuration

```python
# LLM Models
GEMINI_MODEL = "gemini-1.5-flash"
GROQ_MODEL = "llama-3.1-70b-versatile"

# Embedding Model
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Audio Models
STT_MODEL = "whisper-large-v3"
TTS_LANGUAGE = "en"
```

## ğŸ¨ UI Components

### ğŸ”® Orb Component
- **Visual States**: Idle, Listening, Speaking
- **Particle Animation**: Galaxy-like dot patterns
- **Color Themes**: Blue (idle), Red (recording), Purple (thinking)

### ğŸ¤ Microphone Button
- **State Indicators**: Color-coded backgrounds
- **Animations**: Scale, glow, and pulse effects
- **Accessibility**: ARIA labels and keyboard support

### â• Upload Button
- **Drag & Drop**: File drop zone
- **Progress Indication**: Loading animations
- **Validation**: PDF file type checking

### ğŸ’¬ Subtitle Display
- **Dynamic Text**: Auto-sizing and positioning
- **Thinking Indicator**: Animated dots
- **Fade Animations**: Smooth transitions

## ğŸ”„ State Management

### ğŸ“Š Voice Assistant Hook

```javascript
const {
  isListening,    // Recording state
  subtitle,       // Display text
  error,          // Error messages
  busy,          // Processing state
  fileId,        // Uploaded file ID
  startListening, // Start recording
  stopListening,  // Stop recording
  setFileId      // Set active document
} = useVoiceAssistant()
```

### ğŸ”„ Conversation Flow

1. **PDF Upload** â†’ Document Processing â†’ Embedding Storage
2. **Voice Input** â†’ Speech Recognition â†’ Text Processing
3. **AI Processing** â†’ Context Retrieval â†’ Response Generation
4. **Voice Output** â†’ Text-to-Speech â†’ Audio Playback

## ğŸ“± Screenshots

### ğŸ  Main Interface
![Main Interface](screenshots/main-interface.png)
*Clean, modern interface with orb visualization*

### ğŸ¤ Voice Recording
![Voice Recording](screenshots/voice-recording.png)
*Recording state with red microphone indicator*

### ğŸ’­ Thinking State
![Thinking State](screenshots/thinking-state.png)
*Processing with animated thinking indicator*

### ğŸ“„ PDF Upload
![PDF Upload](screenshots/pdf-upload.png)
*Drag and drop file upload interface*

## ğŸš€ Deployment

### ğŸ³ Docker Deployment

1. **Create Docker Compose**
```yaml
version: '3.8'
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=mysql+mysqlconnector://user:pass@db:3306/ai_assistant
    depends_on:
      - db
  
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
  
  db:
    image: mysql:8.0
    environment:
      MYSQL_DATABASE: ai_assistant
      MYSQL_ROOT_PASSWORD: rootpassword
    volumes:
      - mysql_data:/var/lib/mysql

volumes:
  mysql_data:
```

2. **Deploy**
```bash
docker-compose up -d
```

### â˜ï¸ Cloud Deployment

- **Frontend**: Vercel, Netlify, or AWS S3
- **Backend**: Railway, Heroku, or AWS ECS
- **Database**: PlanetScale, AWS RDS, or Google Cloud SQL
- **Vector DB**: Pinecone or Weaviate Cloud

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### ğŸ“ Development Guidelines

- Follow ESLint and Prettier configurations
- Write meaningful commit messages
- Add tests for new features
- Update documentation as needed
- Ensure all tests pass before submitting

### ğŸ› Bug Reports

Please use the GitHub Issues template and include:
- Browser and OS information
- Steps to reproduce
- Expected vs actual behavior
- Screenshots if applicable

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **OpenAI** for Whisper model inspiration
- **Google** for Gemini LLM and TTS services
- **Groq** for ultra-fast inference
- **ChromaDB** for vector storage
- **React** and **FastAPI** communities

## ğŸ“ Support

- ğŸ“§ Email: support@yourproject.com
- ğŸ’¬ Discord: [Join our community](https://discord.gg/yourserver)
- ğŸ“š Docs: [Documentation site](https://docs.yourproject.com)
- ğŸ› Issues: [GitHub Issues](https://github.com/yourusername/ai-voice-assistant/issues)

---

<div align="center">

**Built with â¤ï¸ by [Your Team Name]**

[â­ Star us on GitHub](https://github.com/yourusername/ai-voice-assistant) â€¢ [ğŸ¦ Follow on Twitter](https://twitter.com/yourhandle) â€¢ [ğŸ’¼ LinkedIn](https://linkedin.com/company/yourcompany)

</div>